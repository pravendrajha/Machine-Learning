from sklearn.ensemble import RandomForestClassifier
cls = RandomForestClassifier(n_estimators =100,criterion='entropy',random_state=0)
cls.fit(X_train,y_train)


false Positive ( predicated + but actual Negative) -->Error 1 
false negative ( predicated Negative but actual Positive)  -->Error 2 --> ( Dangours)

Y predicated and Y actual is same 

False Negative ( type 2 )  --> Actually Postive predictive negative 

Accuracy --> Correct /Total

Error --> Wrong/Total

Cummulative Accuracy Profile --> 

Receiver Operating Charcterstics ->

ar/ap if value is approching 1 , its good else notgood 

if 90-100 % then its shows overfitting 


Kmeans :-
Clusters of data set,it can work for multiple dimensional 
1-->Number of cluster K 
2-->sleect at randon K points , centrioids the cluster 
3-->assign each data point to closets data point to closet centroid  ( ecludian distance)
4-->comute and place the new centroid of each cluster
5-->reassign each data to new closet the data 


Random Initialization Trape ==>

select the centrioids at diffrent place and scatter data 

bad random initialization -->

right choice of kluster in K mean alogorithm , Optimal number of clusters 


Elbow :-

from sklearn.cluster import Kmeans 
wccs =[]

for i in range[1,11] :
 kmeans = Kmeans(n_clusters=i,init ='k-menas++', max_iter=300,n_inti=10,random_state=0)
 kmeans.fit(X)
 wccs.append(kmeans.inertia_)

plt.plot( range[1,11],wccs)
plt.plot( range[1,11],wccs)
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCCS')
plt.show()
